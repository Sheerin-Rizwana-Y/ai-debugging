import json
from llm.llm_client import query_llm
from models.prompt_log_model import log_prompt_chain

def run_prompt_chain(code: str, ml_result: dict, user_id=None) -> dict:
    context = {
        "code": code,
        "language": ml_result.get("language", "Unknown"),
        "error_type": ml_result.get("error_type", "Unknown")
    }

    # üß† Step 1: Ask LLM to identify each error as a structured JSON object
    prompt_1 = (
        f"The following code is written in {context['language']}:\n\n{context['code']}\n\n"
        "Identify all errors. For each error, output a JSON object like:\n"
        '{"line": <number>, "type": "<ErrorType>", "message": "<explanation>"}\n'
        "Return a JSON array of such objects. Do not include any extra commentary."
    )
    diagnosis = query_llm(prompt_1)

    # üõ°Ô∏è Try to parse structured output ‚Äî fallback to empty list if invalid
    try:
        parsed_diagnosis = json.loads(diagnosis)
        if not isinstance(parsed_diagnosis, list):
            raise ValueError("Diagnosis is not a JSON array")
    except Exception:
        parsed_diagnosis = []
        diagnosis = "‚ùå LLM failed to return valid JSON diagnosis."

    # üõ†Ô∏è Step 2: Request corrected code
    prompt_2 = (
        f"The following errors were found in the code:\n{parsed_diagnosis}\n\n"
        "Rewrite the complete corrected code, applying all fixes. Return the code inside triple backticks, with no other text."
    )
    fix = query_llm(prompt_2)
    if not fix or "LLM Error" in fix:
        fix = "‚ùå LLM failed to generate a corrected version."

    # üí¨ Step 3: Request explanation of the fix
    prompt_3 = (
        f"{fix}\nExplain how these changes resolved the listed errors."
    )
    explanation = query_llm(prompt_3)
    if not explanation or "LLM Error" in explanation:
        explanation = "‚ùå LLM failed to explain the correction."

    # üì¶ Combine results
    response_chain = {
        "diagnosis": diagnosis,
        "diagnosis_json": parsed_diagnosis,
        "fix": fix,
        "explanation": explanation
    }

    # üóÇÔ∏è Optional logging for fine-tuning pipeline
    if user_id:
        try:
            log_prompt_chain(
                user_id=user_id,
                context=context,
                prompts=[prompt_1, prompt_2, prompt_3],
                responses=response_chain
            )
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to log prompt chain: {e}")

    return response_chain
